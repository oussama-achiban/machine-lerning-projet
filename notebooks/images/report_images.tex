\documentclass[11pt,a4paper]{article}

% ================== Packages ==================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{sectsty}
\usepackage{float}
\usepackage{color}
\usepackage{cite}

% ================== Page Setup ==================
\geometry{margin=1in}
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}
\setlength{\headheight}{34pt}

\pagestyle{fancy}
\fancyhf{}
\lhead{\includegraphics[height=0.9cm]{UCA-FSSM.png}}
\chead{\textit{School Electricity Access Analysis}}
\rhead{\thepage}

\allsectionsfont{\sffamily\bfseries\large}

% ================== Title ==================
\title{
\includegraphics[width=1\textwidth]{UCA-FSSM.png}\\[0.3cm]
\small \\[0.3cm]
\textbf{Global School Electricity Access Analysis}\\
\vspace{0.3cm}
\large Machine Learning and Data Science Project\\
\normalsize Master ISI -- Intelligent Information Systems
}

\author{
Oussama Achiban\\
\textit{Master ISI}
}

\date{February 13, 2026}

\begin{document}

\maketitle

% ================== Abstract ==================
\begin{abstract}
This report presents a comprehensive machine learning analysis of global school electricity access. The visual outputs summarize the complete workflow: exploratory data analysis, dimensionality reduction, unsupervised clustering, and supervised classification. The objective is to identify structural patterns in school infrastructure data and evaluate predictive performance for electricity access categories.
\end{abstract}

\textbf{Keywords:} Machine Learning, Clustering, Classification, Dimensionality Reduction, School Infrastructure, MLflow

\textbf{GitHub Repository:} \href{https://github.com/oussama-achiban/machine-lerning-projet}{https://github.com/oussama-achiban/machine-lerning-projet}

\newpage
\tableofcontents
\newpage

% ================== Introduction ==================
\section{Introduction}
Access to electricity in schools is a key enabler of educational quality, digital inclusion, and infrastructure modernization. Despite global progress, disparities remain across regions and development levels. This report documents the visual results generated by the notebook pipeline and provides an interpretable summary of the modeling process.

The main objectives are:
\begin{enumerate}
    \item Explore the statistical structure of the dataset;
    \item Reduce dimensionality while preserving informative variation;
    \item Identify natural groups of schools using clustering;
    \item Compare classification models for predictive performance;
    \item Track experiments and model runs with MLflow.
\end{enumerate}

% ================== Dataset ==================
\section{Dataset Description}
\subsection{Data Overview}
The notebook loads a dataset of \textbf{500 rows} and \textbf{7 columns}: \texttt{school\_id}, \texttt{year}, \texttt{access\_rate}, \texttt{infrastructure\_score}, \texttt{region}, \texttt{development\_level}, and \texttt{investment}. Missing values are zero for all columns.

\subsection{Data Tables}

\begin{table}[H]
\centering
\small
\begin{tabular}{ccccccc}
\toprule
school\_id & year & access\_rate & infrastructure\_score & region & development\_level & investment \\
\midrule
0 & 2005 & 73.1648 & 31 & Asia & Low & 97722 \\
1 & 2018 & 12.7690 & 45 & Asia & High & 48684 \\
2 & 2013 & 25.0016 & 15 & Asia & High & 31111 \\
3 & 2009 & 58.0544 & 67 & Africa & Medium & 64936 \\
4 & 2006 & 86.7117 & 36 & Europe & Low & 23614 \\
\bottomrule
\end{tabular}
\caption{First five rows of the dataset (from notebook output)}
\label{tab:data_head}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Variable & Mean & Std & Min & Max \\
\midrule
year & 2009.0840 & 6.6833 & 1999.0000 & 2020.0000 \\
access\_rate & 50.9118 & 27.8599 & 0.2595 & 99.7693 \\
infrastructure\_score & 49.8740 & 29.2849 & 0.0000 & 99.0000 \\
investment & 48982.8080 & 28865.1841 & 1162.0000 & 99989.0000 \\
\bottomrule
\end{tabular}
\caption{Descriptive statistics for key numeric variables}
\label{tab:desc_stats}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{lc}
\toprule
Column & Missing values \\
\midrule
school\_id & 0 \\
year & 0 \\
access\_rate & 0 \\
infrastructure\_score & 0 \\
region & 0 \\
development\_level & 0 \\
investment & 0 \\
\bottomrule
\end{tabular}
\caption{Missing values by column}
\label{tab:missing_values}
\end{table}

\subsection{Exploratory Data Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{eda_numeric_distributions.png}
\caption{Numeric feature distributions}
\label{fig:eda_numeric}
\end{figure}

Figure~\ref{fig:eda_numeric} shows the empirical distributions of quantitative attributes. It highlights variability and potential skewness in infrastructure and investment-related variables, which motivates robust preprocessing and scaling.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{eda_correlation_heatmap.png}
\caption{Correlation heatmap of numeric features}
\label{fig:eda_corr}
\end{figure}

Figure~\ref{fig:eda_corr} presents pairwise correlations among numeric features. Strong linear relations indicate redundancy and justify the use of dimensionality reduction methods.

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{eda_category_counts_development_level.png}
\caption{Category counts for development level}
\label{fig:eda_dev}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{eda_category_counts_region.png}
\caption{Category counts for region}
\label{fig:eda_region}
\end{figure}

Figures~\ref{fig:eda_dev} and~\ref{fig:eda_region} summarize categorical balance. They provide context for class distribution and potential sampling asymmetries across geopolitical groups.

% ================== Methodology ==================
\section{Methodology}
\subsection{Preprocessing}
The processing pipeline includes duplicate handling, missing-value checks, categorical encoding, and feature standardization. Standardization is applied as:
\[
z = \frac{x - \mu}{\sigma}
\]
This transformation ensures comparable scales across features before distance-based methods such as PCA, K-Means, and t-SNE.

\subsection{Modeling Strategy}
The workflow combines:
\begin{itemize}
    \item Dimensionality reduction to extract compact latent representations;
    \item Clustering to identify natural segments of schools;
    \item Classification to predict electricity access categories.
\end{itemize}

% ================== Dimensionality Reduction ==================
\section{Dimensionality Reduction}

\subsection{Principal Component Analysis (PCA)}
PCA projects correlated variables into orthogonal components:
\[
\mathbf{X}_{\text{reduced}} = \mathbf{X} \cdot \mathbf{W}
\]

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{dimred_pca_cumulative_variance.png}
\caption{PCA cumulative explained variance}
\label{fig:pca_cumvar}
\end{figure}

Figure~\ref{fig:pca_cumvar} shows how quickly variance accumulates with the number of components and supports selecting a compact representation with limited information loss.

\subsection{2D Projections (PCA, t-SNE, NMF)}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{dimred_2d_projections.png}
\caption{PCA, t-SNE and NMF 2D projections}
\label{fig:dimred_2d}
\end{figure}

Figure~\ref{fig:dimred_2d} compares linear and nonlinear embeddings. The visualization reveals neighborhood structures and potential separability patterns that guide clustering and classification interpretation.

% ================== Clustering ==================
\section{Clustering Analysis}

\subsection{K-Means Model Selection}
The K-Means objective is:
\[
J = \sum_{i=1}^{k} \sum_{x \in C_i} \|x - \mu_i\|^2
\]

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{clustering_elbow_silhouette.png}
\caption{K-Means elbow curve and silhouette scores}
\label{fig:cluster_selection}
\end{figure}

Figure~\ref{fig:cluster_selection} combines inertia and silhouette diagnostics to support a balanced choice of cluster count between compactness and separation.

\subsection{Cluster Visualization in PCA Space}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{clustering_assignments_pca.png}
\caption{Cluster assignments projected on PCA space}
\label{fig:cluster_pca}
\end{figure}

Figure~\ref{fig:cluster_pca} displays final assignments in reduced space. Cluster overlap and boundaries provide insights into gradual transitions between school profiles.

% ================== Classification ==================
\section{Classification Models}

Several classical supervised models are trained and compared using standard metrics (accuracy, precision, recall, and F1-score).

\begin{table}[H]
\centering
\small
\begin{tabular}{lcccc}
\toprule
Model & Accuracy & Precision & Recall & F1-Score \\
\midrule
Decision Tree & 1.00 & 1.0000 & 1.00 & 1.0000 \\
AdaBoost & 1.00 & 1.0000 & 1.00 & 1.0000 \\
Random Forest & 1.00 & 1.0000 & 1.00 & 1.0000 \\
Gradient Boosting & 1.00 & 1.0000 & 1.00 & 1.0000 \\
Logistic Regression & 0.96 & 0.9423 & 0.98 & 0.9608 \\
SVM & 0.95 & 0.9245 & 0.98 & 0.9515 \\
KNN & 0.91 & 0.8727 & 0.96 & 0.9143 \\
\bottomrule
\end{tabular}
\caption{Model performance summary extracted from notebook output}
\label{tab:classification_results}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{classification_model_comparison.png}
\caption{Comparison of classical model performance}
\label{fig:classif_models}
\end{figure}

Figure~\ref{fig:classif_models} summarizes comparative performance and highlights the strongest candidates for robust prediction of electricity access classes.

% ================== MLflow ==================
\section{MLflow Tracking Summary}

The notebook logs experiments to:
\begin{itemize}
    \item Tracking URI: \texttt{file:///C:/Users/oussa/Downloads/machine-learning-project/mlruns}
    \item Experiment name: \texttt{School\_Electricity\_Access\_ML}
    \item Experiment ID: \texttt{905967612892822029}
    \item Total recorded runs: \texttt{11}
\end{itemize}

\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{llll}
\toprule
Run ID & Status & Start Time (UTC) & Run Name \\
\midrule
f0605619991c4ddaaba00f64dc0d65ee & FINISHED & 2026-02-13 11:08:01.942 & Final\_Summary \\
620ed22a64d04fbb9693360dd8f9c02c & FINISHED & 2026-02-13 11:07:58.945 & Classical\_Models \\
64f76dc9530f41cdb62bf1291b6a06cc & FINISHED & 2026-02-13 11:07:53.473 & Clustering \\
027aad30dcce4c5eb0f4b31fef1e969d & FINISHED & 2026-02-13 11:07:46.732 & Dimensionality\_Reduction \\
568a377fbceb4434a9a914d0d818ba4a & FINISHED & 2026-02-13 11:07:46.506 & Preprocessing \\
\bottomrule
\end{tabular}
\caption{Latest MLflow runs shown in the notebook}
\label{tab:mlflow_runs}
\end{table}

Best model reported by the notebook summary is \textbf{Decision Tree} with metrics:
\texttt{accuracy=1.0, precision=1.0, recall=1.0, f1=1.0}.

A warning also appears in outputs about a malformed local experiment folder:
\texttt{mlruns/1/meta.yaml does not exist}. This does not block listing the active experiment but should be cleaned for consistency.

To open the tracking UI:
\begin{verbatim}
mlflow ui --backend-store-uri file:///C:/Users/oussa/Downloads/machine-learning-project/mlruns
\end{verbatim}

% ================== Results and Discussion ==================
\section{Results and Discussion}
The complete pipeline reveals coherent patterns across stages. EDA confirms heterogeneous distributions and meaningful inter-feature correlations. Dimensionality reduction preserves global structure while exposing low-dimensional geometry. Clustering identifies interpretable groups, and classification quantifies predictive capacity with clear performance differences among algorithms.

From a policy perspective, these results support data-driven targeting of infrastructure interventions by identifying profiles associated with lower access outcomes.

% ================== Conclusion ==================
\section{Conclusion}
This report consolidates the figures, tables, and experiment tracking outputs generated by the notebook workflow into a structured machine learning narrative. The combination of statistical exploration, representation learning, clustering, supervised prediction, and MLflow tracking provides a reliable framework for analyzing school electricity access and informing strategic decision-making.

% ================== References ==================
\section*{References}
\begin{thebibliography}{99}

\bibitem{Breiman2001}
Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5--32.

\bibitem{Chen2016}
Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. \textit{KDD}.

\bibitem{Pedregosa2011}
Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. \textit{JMLR}.

\bibitem{mlflow}
Zaharia, M., et al. (2018). Accelerating the Machine Learning Lifecycle with MLflow.

\end{thebibliography}

\end{document}

